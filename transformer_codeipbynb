import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, mean_squared_error, classification_report
from sklearn.dummy import DummyClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Dense, Flatten, MaxPooling1D, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.utils import shuffle

# Import other specific models (replace with actual implementations)
# Assuming the following imports for EEGNet, InceptionTime, Xception, ViT, etc.
# from eegnet import EEGNet
# from inception_time import InceptionTime
# from xception import Xception
# from vit import ViT

# Step 1: Load the dataset
dataset_path = "EEGEYENet.csv"  # Replace with your dataset path
data = pd.read_csv(dataset_path)

# Step 2: Preprocess the dataset
X = data.iloc[:, :-1].values  # Features
y = data.iloc[:, -1].values   # Target labels

# Shuffle and split
X, y = shuffle(X, y, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features for traditional ML models
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 3: Define Models
models = {
    "Naive Guessing": DummyClassifier(strategy="most_frequent"),
    "KNN": KNeighborsClassifier(),
    "RBF SVR": SVR(kernel="rbf"),
    "Linear Regression": LinearRegression(),
    "Ridge Regression": Ridge(),
    "Lasso Regression": Lasso(),
    "Elastic Net": ElasticNet(),
    "Random Forest": RandomForestClassifier(),
    "Gradient Boost": GradientBoostingClassifier(),
    "AdaBoost": AdaBoostClassifier(),
    "XGBoost": XGBClassifier(),
}

# Example for deep learning models
def build_simple_cnn(input_shape):
    model = Sequential([
        Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape),
        MaxPooling1D(pool_size=2),
        Flatten(),
        Dense(64, activation='relu'),
        Dropout(0.5),
        Dense(1, activation='sigmoid')  # Adjust based on output classes
    ])
    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Add deep learning models
input_shape = (X_train.shape[1], 1)
models.update({
    "CNN": build_simple_cnn((X_train.shape[1], 1)),
    "EEGNet": "Add EEGNet implementation here",
    "InceptionTime": "Add InceptionTime implementation here",
    "Xception": "Add Xception implementation here",
    "ViT-Base": "Add ViT implementation here",
    "ViT-Base Pre-trained": "Add Pre-trained ViT implementation here",
    "EEGViT": "Add EEGViT implementation here",
    "EEGViT Pre-trained": "Add Pre-trained EEGViT implementation here"
})

# Step 4: Train and Evaluate Models
results = {}

for name, model in models.items():
    print(f"Training {name}...")
    
    if name in ["CNN", "EEGNet", "InceptionTime", "Xception", "ViT-Base", "EEGViT"]:
        # Deep learning models
        X_train_dl = np.expand_dims(X_train, axis=-1)
        X_test_dl = np.expand_dims(X_test, axis=-1)
        model.fit(X_train_dl, y_train, epochs=10, batch_size=32, verbose=0)
        y_pred = (model.predict(X_test_dl) > 0.5).astype(int)
        accuracy = accuracy_score(y_test, y_pred)
    else:
        # Traditional ML models
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        if hasattr(model, "predict_proba"):
            accuracy = accuracy_score(y_test, y_pred)
        else:
            mse = mean_squared_error(y_test, y_pred)
            accuracy = -mse  # Lower MSE is better, inverting for comparison
        
    results[name] = accuracy

# Step 5: Display Results
sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)

print("\nModel Performance (sorted by accuracy):")
for name, score in sorted_results:
    print(f"{name}: {score:.4f}")

best_model = sorted_results[0]
print(f"\nBest Model: {best_model[0]} with score: {best_model[1]:.4f}")